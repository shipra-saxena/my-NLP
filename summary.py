# -*- coding: utf-8 -*-
"""Summary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V9XaU59nVqHFAeEIfr9pZtHVvDhKtfRj
"""

import nltk
nltk.download('stopwords')
import pandas as pd 
import numpy as np
from nltk.corpus import stopwords
from nltk.cluster.util import cosine_distance
import networkx as nx

length = int(input("enter percentage you want summary"))

N = int((10*length)/100)
print("length of summary is %d lines" %N)

def read_file():
  
  file = open("sahi.txt", 'r', encoding = 'ascii', errors='ignore')
  filedata = file.readlines()
  article = filedata[0].split(". ")
  sentences = []
  for sentence in article:
    print(sentence)
    sentences.append(sentence.replace("[^a-zA-Z]", " ").split(" "))
  sentences.pop() 
  return sentences

def similarity(sent1, sent2, stopwords=None):
    if stopwords is None:
        stopwords = []
 
    sent1 = [w.lower() for w in sent1]
    sent2 = [w.lower() for w in sent2]
 
    all_words = list(set(sent1 + sent2))
 
    vector1 = [0] * len(all_words)
    vector2 = [0] * len(all_words)
 
    # build the vector for the first sentence
    for w in sent1:
        if w in stopwords:
            continue
        vector1[all_words.index(w)] += 1
 
    # build the vector for the second sentence
    for w in sent2:
        if w in stopwords:
            continue
        vector2[all_words.index(w)] += 1
 
    return 1 - cosine_distance(vector1, vector2)

def generate_similarity_matrix(sentences, stop_words):
   
    similarity_matrix = np.zeros((len(sentences), len(sentences)))
 
    for i in range(len(sentences)):
        for j in range(len(sentences)):
            if i == j: 
                continue 
            similarity_matrix[i][j] = sentence_similarity(sentences[i], sentences[j], stop_words)

    return similarity_matrix

def summary():
  
    stop_words = stopwords.words('english')
    summarized = []

    sentences =  read_file()

    sentence_similarity_martix = generate_similarity_matrix(sentences, stop_words)

    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)
    scores = nx.pagerank(sentence_similarity_graph)

    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)      

    for i in range(N):
      summarized.append(" ".join(ranked_sentence[i][1]))

    print("*"*50)
    print("Summarize Text: \n", ". ".join(summarized))

summary()